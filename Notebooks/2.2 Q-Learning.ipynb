{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45de3e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fd4d95",
   "metadata": {},
   "source": [
    "#### Empecemos definiendo nuestro entorno."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d9f0f8",
   "metadata": {},
   "source": [
    "Las recompensas son 0 para todos los estados, excepto para el estado de más a la izquierda y el de más a la derecha, que tienen recompensas de -5 y +5 respectivamente. También definimos una lista que define si un estado es final/terminal o no. Y por último creamos la lista de variables llamada Q_values, donde guardaremos los valores-Q para todos los pares de estados y acciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b193c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_rewards = [-5, 0, 0, 0, 0, 0, 5]\n",
    "final_state = [True, False, False, False, False, False, True]\n",
    "Q_values = [[0.0, 0.0], \n",
    "            [0.0, 0.0],\n",
    "            [0.0, 0.0],\n",
    "            [0.0, 0.0],\n",
    "            [0.0, 0.0],\n",
    "            [0.0, 0.0],\n",
    "            [0.0, 0.0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaf2369",
   "metadata": {},
   "source": [
    "Ahora crearemos una función que escoja una acción usando la política ε-voraz para un estado que pasaremos como parámetro. También crearemos una función que ejerza de entorno. Le pasaremos el estado y la acción seleccionada por el agente, y nos devolverá la recompensa r y el siguiente estado s’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4848b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_epsilon_greedy_action(epsilon, state):\n",
    "    \"\"\"Take random action with probability epsilon, else take best action.\"\"\"\n",
    "    result = np.random.uniform()\n",
    "    if result < epsilon:\n",
    "        return np.random.randint(0, 2)\n",
    "    else:\n",
    "        return np.argmax(Q_values[state])\n",
    "\n",
    "def apply_action(state, action):\n",
    "    \"\"\"Applies the selected action and get reward and next state.\n",
    "    Action 0 means move to the left and action 1 means move to the right.\"\"\"\n",
    "    if action == 0:\n",
    "        next_state = state-1\n",
    "    else:\n",
    "        next_state = state+1\n",
    "    return state_rewards[next_state], next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c3a44b",
   "metadata": {},
   "source": [
    "Por último, decidimos varios hiperparámetros y ejecutamos el algoritmo que aprende usando el algoritmo de Q-Learning y la ecuación de Bellman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9727ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "epsilon = 0.2\n",
    "discount = 0.9 \n",
    "\n",
    "for episode in range(num_episodes+1):\n",
    "    initial_state = 3 \n",
    "    state = initial_state\n",
    "    while not final_state[state]:\n",
    "        action = select_epsilon_greedy_action(epsilon, state)\n",
    "        reward, next_state = apply_action(state, action)\n",
    "        if final_state[next_state]:\n",
    "            Q_values[state][action] = reward\n",
    "        else:\n",
    "            Q_values[state][action] = reward + discount * max(Q_values[next_state])\n",
    "        state = next_state\n",
    "\n",
    "print('Final Q-values are:')\n",
    "print(Q_values)\n",
    "action_dict = {0:'left', 1:'right'}\n",
    "state = 0\n",
    "for state, Q_vals in enumerate(Q_values):\n",
    "    print('Best action for state {} is {}'.format(state, \n",
    "                                             action_dict[np.argmax(Q_vals)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216e395f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
