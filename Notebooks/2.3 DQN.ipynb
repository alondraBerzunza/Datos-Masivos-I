{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110d0c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !apt-get update\n",
    "# !apt-get install -y xvfb python-opengl ffmpeg\n",
    "# !pip install pyglet==1.3.2\n",
    "# !pip install gym pyvirtualdisplay\n",
    "# !pip install torch\n",
    "# !pip install xvfbwrapper\n",
    "# pip install moviepy\n",
    "import gym\n",
    "from gym.wrappers.record_video import RecordVideo\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8626ec91",
   "metadata": {},
   "source": [
    "Construimos el agente DQN y algunas funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ab3188",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "num_features = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "print('Number of state features: {}'.format(num_features))\n",
    "print('Number of possible actions: {}'.format(num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43612312",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3572c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Dense neural network class.\"\"\"\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_inputs, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.out = nn.Linear(32, num_actions)\n",
    "\n",
    "    def forward(self, states):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        x = F.relu(self.fc1(states))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.out(x)\n",
    "\n",
    "main_nn = DQN(num_features, num_actions).to(device)\n",
    "target_nn = DQN(num_features, num_actions).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(main_nn.parameters(), lr=1e-4)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b95daad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \"\"\"Experience replay buffer that samples uniformly.\"\"\"\n",
    "    def __init__(self, size, device=\"cpu\"):\n",
    "        \"\"\"Initializes the buffer.\"\"\"\n",
    "        self.buffer = deque(maxlen=size)\n",
    "        self.device = device\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        idx = np.random.choice(len(self.buffer), num_samples)\n",
    "        for i in idx:\n",
    "            elem = self.buffer[i]\n",
    "            state, action, reward, next_state, done = elem\n",
    "            states.append(np.array(state, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward)\n",
    "            next_states.append(np.array(next_state, copy=False))\n",
    "            dones.append(done)\n",
    "        states = torch.as_tensor(np.array(states), device=self.device)\n",
    "        actions = torch.as_tensor(np.array(actions), device=self.device)\n",
    "        rewards = torch.as_tensor(\n",
    "            np.array(rewards, dtype=np.float32), device=self.device)\n",
    "        next_states = torch.as_tensor(np.array(next_states), device=self.device)\n",
    "        dones = torch.as_tensor(np.array(dones, dtype=np.float32), device=self.device)\n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d628b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_epsilon_greedy_action(state, epsilon):\n",
    "    \"\"\"Take random action with probability epsilon, else take best action.\"\"\"\n",
    "    result = np.random.uniform()\n",
    "    if result < epsilon:\n",
    "        return env.action_space.sample() \n",
    "    else:\n",
    "        qs = main_nn(state).cpu().data.numpy()\n",
    "        return np.argmax(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f094a044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(states, actions, rewards, next_states, dones):\n",
    "    \"\"\"Perform a training iteration on a batch of data sampled from the experience\n",
    "    replay buffer.\"\"\"\n",
    "    max_next_qs = target_nn(next_states).max(-1).values\n",
    "    target = rewards + (1.0 - dones) * discount * max_next_qs\n",
    "    qs = main_nn(states)\n",
    "    action_masks = F.one_hot(actions, num_actions)\n",
    "    masked_qs = (action_masks * qs).sum(dim=-1)\n",
    "    loss = loss_fn(masked_qs, target.detach())\n",
    "    #nn.utils.clip_grad_norm_(loss, max_norm=10)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473741ba",
   "metadata": {},
   "source": [
    "Corremos el algoritmo DQN y observamos como aprende el algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b76f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparametros.\n",
    "num_episodes = 1000\n",
    "epsilon = 1.0\n",
    "batch_size = 32\n",
    "discount = 0.99\n",
    "buffer = ReplayBuffer(100000, device=device)\n",
    "cur_frame = 0\n",
    "\n",
    "# Empieza a entrenar. Juega una vez y luego entrena con un batch.\n",
    "last_100_ep_rewards = []\n",
    "for episode in range(num_episodes+1):\n",
    "    state = env.reset()[0].astype(np.float32)\n",
    "    ep_reward, done = 0, False\n",
    "    while not done:\n",
    "        state_in = torch.from_numpy(np.expand_dims(state, axis=0)).to(device)\n",
    "        action = select_epsilon_greedy_action(state_in, epsilon)\n",
    "        next_state, reward, done, info = env.step(action)[:4]\n",
    "        next_state = next_state.astype(np.float32)\n",
    "        ep_reward += reward\n",
    "        # Guardamos para reproducir la experiencia.\n",
    "        buffer.add(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        cur_frame += 1\n",
    "        # Copiamos los pesos de main_nn a target_nn.\n",
    "        if cur_frame % 2000 == 0:\n",
    "            target_nn.load_state_dict(main_nn.state_dict())\n",
    "    \n",
    "        # Entrenamos la red neuronal.\n",
    "        if len(buffer) > batch_size:\n",
    "            states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "            loss = train_step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "    if episode < 950:\n",
    "        epsilon -= 0.001\n",
    "\n",
    "    if len(last_100_ep_rewards) == 100:\n",
    "        last_100_ep_rewards = last_100_ep_rewards[1:]\n",
    "    last_100_ep_rewards.append(ep_reward)\n",
    "\n",
    "    if episode % 50 == 0:\n",
    "        print(f'Episode {episode}/{num_episodes}. Epsilon: {epsilon:.3f}.'\n",
    "          f' Reward in last 100 episodes: {np.mean(last_100_ep_rewards):.2f}')\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d14881",
   "metadata": {},
   "source": [
    "Mostrar el resultado del agente DQN entrenado en el entorno Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d735ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_video():\n",
    "    \"\"\"Enables video recording of gym environment and shows it.\"\"\"\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Video not found\")\n",
    "\n",
    "def wrap_env(env):\n",
    "    env = RecordVideo(env, './video', episode_trigger = lambda episode_number: True)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1bd61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = wrap_env(gym.make('CartPole-v0'))\n",
    "state = env.reset()[0]\n",
    "done = False\n",
    "ep_rew = 0\n",
    "while not done:\n",
    "    env.render()\n",
    "    state = state.astype(np.float32)\n",
    "    state = torch.from_numpy(np.expand_dims(state, axis=0)).to(device)\n",
    "    action = select_epsilon_greedy_action(state, epsilon=0.01)\n",
    "    state, reward, done, info = env.step(action)[0]\n",
    "    ep_rew += reward\n",
    "print('Return on this episode: {}'.format(ep_rew))\n",
    "env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c3c8fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
